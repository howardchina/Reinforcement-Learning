# Chapter 2 Exercises
### Exercise 2.1
**å¦‚æœæ˜¯*Îµ-è´ªå©ª*åŠ¨ä½œé€‰æ‹©ï¼Œå¯¹äºä¸¤ä¸ªåŠ¨ä½œä¸”Îµ = 0.5çš„æƒ…å†µï¼Œè´ªå©ªåŠ¨ä½œè¢«é€‰æ‹©çš„æ¦‚ç‡æ˜¯ï¼Ÿ**

æ¦‚ç‡æ˜¯75%ï¼Œç”±ä¸¤éƒ¨åˆ†æ„æˆï¼š
- 50%æ¦‚ç‡æºè‡ªé€‰æ‹©è´ªå©ªåŠ¨ä½œï¼Œå³1-Îµ = 1-0.5 = 0.5
- 25%æ¦‚ç‡æºè‡ªéšæœºåŠ¨ä½œï¼Œå³Îµ*(1/2) = 0.25


### Exercise 2.2: *èµŒåšæœºçš„ä¾‹å­*
**è€ƒè™‘ä¸€ä¸ª*k*=4çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œå‡ åº§1ã€2ã€3ã€4ã€‚å°†èµŒåšæœºç®—æ³•åº”ç”¨äºè¿™ä¸ªé—®é¢˜ï¼Œç®—æ³•ä½¿ç”¨Îµ-è´ªå©ªåŠ¨ä½œé€‰æ‹©ï¼ŒåŸºäºé‡‡æ ·å¹³å‡çš„åŠ¨ä½œä»·å€¼ä¼°è®¡ï¼Œåˆå§‹ä¼°è®¡Qâ‚(*a*) = 0ï¼Œ $\forall a$ ã€‚å‡è®¾åˆå§‹åŠ¨ä½œåºåˆ—å’Œå›æŠ¥æ˜¯Aâ‚ = 1, Râ‚ = -1, Aâ‚‚ = 2, Râ‚‚ = 1, Aâ‚ƒ = 2, Râ‚ƒ = -2, Aâ‚„ = 2, Râ‚„ = 2, Aâ‚… = 3, Râ‚… = 0ã€‚åœ¨æŸäº›æ—¶åˆ»å¯èƒ½å‘ç”ŸÎµçš„æƒ…å½¢ï¼Œå¯¼è‡´ä¸€ä¸ªåŠ¨ä½œè¢«éšæœºé€‰æ‹©ã€‚åœ¨å“ªä¸ªæ—¶åˆ»è¿™ä¸ªæƒ…å½¢è‚¯å®šå‘ç”Ÿäº†ï¼Ÿåœ¨å“ªä¸ªæ—¶åˆ»è¿™ä¸ªæƒ…å½¢å¯èƒ½å‘ç”Ÿäº†ï¼Ÿ**

é€šè¿‡è¡¨æ ¼è®¡ç®—$Q_t(a)$å¯ç¡®å®šè´ªå¿ƒåŠ¨ä½œçš„èŒƒå›´:

   * æ—¶åˆ» 1:  0, 0, 0, 0, 0ï¼Œè´ªå¿ƒåŠ¨ä½œ$\in\{1,2,3,4,5\}$ï¼Œå®é™…åŠ¨ä½œ1ï¼Œå¯èƒ½æ˜¯éšæœº
   * æ—¶åˆ» 2:  -1, 0, 0, 0, 0ï¼Œè´ªå¿ƒåŠ¨ä½œ$\in\{2,3,4,5\}$ï¼Œå®é™…åŠ¨ä½œ2ï¼Œå¯èƒ½æ˜¯éšæœº
   * æ—¶åˆ» 3:  -1, 1, 0, 0, 0ï¼Œè´ªå¿ƒåŠ¨ä½œ$\in\{2\}$ï¼Œå®é™…åŠ¨ä½œ2ï¼Œå¯èƒ½æ˜¯éšæœº
   * æ—¶åˆ» 4:  -1, -0.5, 0, 0, 0ï¼Œè´ªå¿ƒåŠ¨ä½œ$\in\{3,4,5\}$ï¼Œå®é™…åŠ¨ä½œ2ï¼Œä¸€å®šæ˜¯éšæœº
   * æ—¶åˆ» 5:  -1, 0.33, 0, 0, 0ï¼Œè´ªå¿ƒåŠ¨ä½œ$\in\{2\}$ï¼Œå®é™…åŠ¨ä½œ3ï¼Œä¸€å®šæ˜¯éšæœº

### Exercise 2.3:
**In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumupative reward and probability of selecting the best acton? How much better will it be? Express your answer quantitatively.**

The method that will perform best in the long run is Îµ-greedy with Îµ = 0.01. Though it seems like the Îµ-greedy method with Îµ = 0.1 works better for 1000 steps, in the long run Îµ = 0.01 should outperform it. This is true because in the limit when steps tend to infinity, both methods will have the expected rewards converge to the true reward value. Since Îµ = 0.01 has a lower degree of randomness, it will capture a higher amount of reward. In either case any method will have an infinite reward, though the area under the curve will be bigger for Îµ = 0.01 if we were to select a given step much bigger than 1000 and evaluate the curve until that timestamp.

Quantitatively, we can estimate the reward obtained in each step by calculating the expected reward knowing q*(a) (the real value for each action). The expected reward value of a random guess is approximately 0, which means that a random guess' expected reward is 0. For Îµ = 0.1, there's a 90% chance of having a 1.5 reward value, and a .1% of having a random guess, which means a 0 reward. This results in an expected reward of 1.5 * .9 = 1.35.

We can estimate the reward for Îµ = 0.01 in a similar way. For Îµ = 0.01, there's a 99% chance of having a 1.5 reward value, and a .01% of having a random guess with 0 reward. This results in an expected reward of 1.5 * .99 = 1.485. Thus, Îµ-greedy with Îµ = 0.01 results in an expected reward value which is 0.135 units higher than Îµ-greedy with Îµ = 0.1.

### Exercise 2.4:
**If the step-size parameters, ğ›¼<sub>n</sub>, are not constant, then the estimate Q<sub>n</sub> is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?**

[Link to Solution PDF](./2.4.pdf)

<!-- [![alt text](./2.4.pdf  "Soliution to Problem 2.4")] (./2.4.pdf) -->

### Exercise 2.6: *Mysterious Spikes*
**The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandits tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?**

The oscillations in the early part of the optimistic method's curve seem to occur when the estimated values are still optimistic. When the curve increases, it is due to choosing the best bandit (the bandit with the highest expected reward); nevertheless, estimated values are still optimistic (much higher than their expected values). In this context, the best bandit is selected due to its optimistic value at some point during the early part of the curve, but when its estimated value becomes much closer to the real expected value, other decisions with optimistic values are chosen. This makes the curve oscillate during the early stages, until all estimated values aren't as nearly as optimal and becom much closer to the real expected value instead.

This means that for a short number of steps it isn't better to use an optimistic greedy methodology rather than a realistic e-greedy. Additionally, this trick is useful to encourage exploration only during the beginning of execution, and would just be useful for stationary problems.


### Exercise 2.7:
**Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.**


[Link to Solution PDF](./2.7.pdf)

<!-- [![alt text](./2.7.pdf  "Soliution to Problem 2.4")] (./2.7.pdf) -->

